{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:11.240410Z",
     "start_time": "2020-01-20T16:49:09.690831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../modules/transformers/')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:12.205173Z",
     "start_time": "2020-01-20T16:49:11.242123Z"
    }
   },
   "outputs": [],
   "source": [
    "# BERT_PATH = '../data/bert-base-uncased-huggingface-transformer/'\n",
    "# tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v1')\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_sub = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some functions that will be used to preprocess the raw text data into useable Bert inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:12.214433Z",
     "start_time": "2020-01-20T16:49:12.207882Z"
    }
   },
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation_strategy=truncation_strategy)\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title + ' ' + question, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        answer, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :**  \n",
    "encode_plus 's return   \n",
    "{  \n",
    "    input_ids: list[int],  \n",
    "    token_type_ids: list[int] if return_token_type_ids is True (default)  \n",
    "    attention_mask: list[int] if return_attention_mask is True (default)  \n",
    "    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True  \n",
    "    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True  \n",
    "    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:12.239065Z",
     "start_time": "2020-01-20T16:49:12.216024Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:12.310147Z",
     "start_time": "2020-01-20T16:49:12.241639Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_spearmanr()` is used to compute the competition metric for the validation set,   \n",
    "`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:12.371465Z",
     "start_time": "2020-01-20T16:49:12.312752Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:12.446505Z",
     "start_time": "2020-01-20T16:49:12.374154Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "#     config =XLNetConfig() # print(config) to see settings\n",
    "    config =AlbertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "#     xlnet_model = TFAlbertModel.from_pretrained('albert-base-v1', config=config)\n",
    "    xlnet_model = TFAlbertModel.from_pretrained('albert-large-v1')\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = xlnet_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = xlnet_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "\n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Obtain inputs and targets, as well as the indices of the train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:28.121612Z",
     "start_time": "2020-01-20T16:49:12.449082Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:13, 452.46it/s]\n",
      "476it [00:01, 475.04it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T16:49:28.127340Z",
     "start_time": "2020-01-20T16:49:28.123573Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, fold=None):\n",
    "        self.fold = fold\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         tf.saved_model.save(\n",
    "#             self.model, f'../saved_models/huggingface_albert/fold{fold+1}_ep{epoch+1}')\n",
    "        ckpt = tf.train.Checkpoint(model=self.model)\n",
    "        ckpt.save(f'../saved_models/huggingface_albert/fold{fold+1}_ep{epoch+1}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T18:11:32.213276Z",
     "start_time": "2020-01-20T16:49:28.128581Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        FOLD 1\n",
      "        \n",
      "Train on 4863 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "4864/4863 [==============================] - 933s 192ms/sample - loss: 0.4028\n",
      "Epoch 2/3\n",
      "4864/4863 [==============================] - 685s 141ms/sample - loss: 0.3810\n",
      "Epoch 3/3\n",
      "4864/4863 [==============================] - 686s 141ms/sample - loss: 0.3768\n",
      "validation score =  0.35889772785172747\n",
      "\n",
      "        FOLD 3\n",
      "        \n",
      "Train on 4863 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/pooler/kernel:0', 'tf_albert_model/pooler/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "4864/4863 [==============================] - 951s 196ms/sample - loss: 0.4035\n",
      "Epoch 2/3\n",
      "4864/4863 [==============================] - 687s 141ms/sample - loss: 0.3797\n",
      "Epoch 3/3\n",
      "4864/4863 [==============================] - 685s 141ms/sample - loss: 0.3694\n",
      "validation score =  0.38689296309617716\n"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n",
    "\n",
    "valid_preds = []\n",
    "test_preds = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "    # will actually only do 2 folds (out of 5) to manage < 2h\n",
    "    if fold in [0, 2]:\n",
    "        print('''\n",
    "        FOLD {}\n",
    "        '''.format(fold+1))\n",
    "\n",
    "        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "        train_outputs = outputs[train_idx]\n",
    "\n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "        \n",
    "        K.clear_session()\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        with strategy.scope():\n",
    "            model = create_model()\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "            custom = CustomCallback(fold=fold)\n",
    "            model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        model.fit(train_inputs, train_outputs, epochs=3, batch_size=4,callbacks=[custom])\n",
    "        valid_preds.append(model.predict(valid_inputs))\n",
    "        test_preds.append(model.predict(test_inputs))\n",
    "        \n",
    "        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
    "        print('validation score = ', rho_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
