{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.186968Z",
     "start_time": "2020-01-18T16:22:30.712797Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# from bert.tokenization import bert_tokenization as tokenization\n",
    "import sys\n",
    "sys.path.append('../modules/')\n",
    "import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "import gc\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from tensorflow.keras.models import load_model\n",
    "import gc\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.190935Z",
     "start_time": "2020-01-18T16:22:32.188440Z"
    }
   },
   "outputs": [],
   "source": [
    "BERT_PATH = '../data/bert_en_uncased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.274999Z",
     "start_time": "2020-01-18T16:22:32.192530Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(BERT_PATH+'assets/vocab.txt', True)\n",
    "MAX_SEQUENCE_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.396767Z",
     "start_time": "2020-01-18T16:22:32.276806Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.403054Z",
     "start_time": "2020-01-18T16:22:32.399206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (6079, 41)\n",
      "test shape = (476, 11)\n"
     ]
    }
   ],
   "source": [
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.424487Z",
     "start_time": "2020-01-18T16:22:32.405371Z"
    }
   },
   "outputs": [],
   "source": [
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.476376Z",
     "start_time": "2020-01-18T16:22:32.426915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output categories:\n",
      "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "input categories:\n",
      "\t ['question_title', 'question_body', 'answer']\n"
     ]
    }
   ],
   "source": [
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask attention & Padding Tokens (input_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.528210Z",
     "start_time": "2020-01-18T16:22:32.480612Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Tokens by [SEP] (input_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.603924Z",
     "start_time": "2020-01-18T16:22:32.530837Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Tokens to IDs by Tokenizer (input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.679322Z",
     "start_time": "2020-01-18T16:22:32.606288Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.756759Z",
     "start_time": "2020-01-18T16:22:32.681930Z"
    }
   },
   "outputs": [],
   "source": [
    "def _trim_input(title, question, answer, max_sequence_length, \n",
    "                t_max_len=30, q_max_len=239, a_max_len=239):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "        \n",
    "        t = t[:t_new_len]\n",
    "        q = q[:q_new_len]\n",
    "        a = a[:a_new_len]\n",
    "    \n",
    "    return t, q, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conver to Bert inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.767618Z",
     "start_time": "2020-01-18T16:22:32.758736Z"
    }
   },
   "outputs": [],
   "source": [
    "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    \n",
    "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "    input_masks = _get_masks(stoken, max_sequence_length)\n",
    "    input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.802831Z",
     "start_time": "2020-01-18T16:22:32.769718Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        t, q, a = _trim_input(t, q, a, max_sequence_length)\n",
    "\n",
    "        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "        \n",
    "    return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.884257Z",
     "start_time": "2020-01-18T16:22:32.805219Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Spearman's rank correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:32.937017Z",
     "start_time": "2020-01-18T16:22:32.886848Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_spearmanr(trues, preds):\n",
    "    rhos = []\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        rhos.append(\n",
    "            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
    "    return np.mean(rhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:33.014285Z",
     "start_time": "2020-01-18T16:22:32.939790Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n",
    "\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "        self.test_inputs = test_data\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "        self.test_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "        \n",
    "        rho_val = compute_spearmanr(\n",
    "            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n",
    "        \n",
    "        print(\"\\nvalidation rho: %.4f\" % rho_val)\n",
    "        \n",
    "#         self.model.save_weights(f'bert-base.h5py')\n",
    "        \n",
    "        if self.fold is not None:\n",
    "#             self.model.save_weights(f'../saved_models/bert-base-{fold}-{epoch}.h5py')\n",
    "            tf.saved_model.save(self.model, f'../saved_models/hist_bert_ep_fcv/hist_{fold+1}_bert_ep{epoch+1}_fcv')\n",
    "        \n",
    "        self.test_predictions.append(\n",
    "            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:33.104275Z",
     "start_time": "2020-01-18T16:22:33.017114Z"
    }
   },
   "outputs": [],
   "source": [
    "def bert_model():\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n",
    "    input_masks = tf.keras.layers.Input(\n",
    "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n",
    "    input_segments = tf.keras.layers.Input(\n",
    "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n",
    "    \n",
    "    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n",
    "    \n",
    "    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:33.180919Z",
     "start_time": "2020-01-18T16:22:33.106630Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_predict(model, train_data, valid_data, test_data, \n",
    "                      learning_rate, epochs, batch_size, loss_function, fold, times):\n",
    "        \n",
    "    custom_callback = CustomCallback(\n",
    "        valid_data=(valid_data[0], valid_data[1]), \n",
    "        test_data=test_data,\n",
    "        batch_size=batch_size,\n",
    "        fold=fold)\n",
    "#     save_callback = keras.callbacks.ModelCheckpoint(\n",
    "#         filepath='../saved_models/hist_'+str(times)+'_bert', monitor='loss',\n",
    "#         verbose=0, save_best_only=True,\n",
    "#         save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    model.fit(train_data[0], train_data[1], epochs=epochs, \n",
    "              batch_size=batch_size, callbacks=[\n",
    "                  custom_callback,\n",
    "#                   save_callback\n",
    "              ])\n",
    "    \n",
    "    return custom_callback, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:33.191231Z",
     "start_time": "2020-01-18T16:22:33.183569Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:22:33.201278Z",
     "start_time": "2020-01-18T16:22:33.193571Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input & Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:23:00.769801Z",
     "start_time": "2020-01-18T16:22:33.203628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:24, 243.88it/s]\n",
      "476it [00:02, 235.98it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:23:00.805782Z",
     "start_time": "2020-01-18T16:23:00.771600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6079, 512)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(inputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:23:00.809894Z",
     "start_time": "2020-01-18T16:23:00.807900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T16:23:00.819269Z",
     "start_time": "2020-01-18T16:23:00.811034Z"
    }
   },
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T20:05:02.431849Z",
     "start_time": "2020-01-18T16:23:00.822506Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        FOLD 1\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.4039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation rho: nan\n",
      "WARNING:tensorflow:From /home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_1_bert_ep1_fcv/assets\n",
      "5471/5471 [==============================] - 451s 82ms/sample - loss: 0.4039\n",
      "Epoch 2/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3713\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_1_bert_ep2_fcv/assets\n",
      "5471/5471 [==============================] - 437s 80ms/sample - loss: 0.3714\n",
      "Epoch 3/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3558\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_1_bert_ep3_fcv/assets\n",
      "5471/5471 [==============================] - 435s 80ms/sample - loss: 0.3558\n",
      "Epoch 4/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3406\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_1_bert_ep4_fcv/assets\n",
      "5471/5471 [==============================] - 436s 80ms/sample - loss: 0.3406\n",
      "Epoch 5/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3235\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_1_bert_ep5_fcv/assets\n",
      "5471/5471 [==============================] - 435s 80ms/sample - loss: 0.3235\n",
      "\n",
      "        FOLD 2\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.4047\n",
      "validation rho: 0.3499\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_2_bert_ep1_fcv/assets\n",
      "5471/5471 [==============================] - 454s 83ms/sample - loss: 0.4046\n",
      "Epoch 2/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3724\n",
      "validation rho: 0.3761\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_2_bert_ep2_fcv/assets\n",
      "5471/5471 [==============================] - 436s 80ms/sample - loss: 0.3724\n",
      "Epoch 3/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3568\n",
      "validation rho: 0.3838\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_2_bert_ep3_fcv/assets\n",
      "5471/5471 [==============================] - 438s 80ms/sample - loss: 0.3568\n",
      "Epoch 4/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3425\n",
      "validation rho: 0.3894\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_2_bert_ep4_fcv/assets\n",
      "5471/5471 [==============================] - 436s 80ms/sample - loss: 0.3426\n",
      "Epoch 5/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3268\n",
      "validation rho: 0.3923\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_2_bert_ep5_fcv/assets\n",
      "5471/5471 [==============================] - 437s 80ms/sample - loss: 0.3268\n",
      "\n",
      "        FOLD 3\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.4011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_3_bert_ep1_fcv/assets\n",
      "5471/5471 [==============================] - 454s 83ms/sample - loss: 0.4011\n",
      "Epoch 2/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3700\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_3_bert_ep2_fcv/assets\n",
      "5471/5471 [==============================] - 434s 79ms/sample - loss: 0.3699\n",
      "Epoch 3/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3550\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_3_bert_ep3_fcv/assets\n",
      "5471/5471 [==============================] - 437s 80ms/sample - loss: 0.3550\n",
      "Epoch 4/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3395\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_3_bert_ep4_fcv/assets\n",
      "5471/5471 [==============================] - 434s 79ms/sample - loss: 0.3396\n",
      "Epoch 5/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3222\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_3_bert_ep5_fcv/assets\n",
      "5471/5471 [==============================] - 436s 80ms/sample - loss: 0.3222\n",
      "\n",
      "        FOLD 4\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.4023\n",
      "validation rho: 0.3383\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_4_bert_ep1_fcv/assets\n",
      "5471/5471 [==============================] - 455s 83ms/sample - loss: 0.4022\n",
      "Epoch 2/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3706\n",
      "validation rho: 0.3612\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_4_bert_ep2_fcv/assets\n",
      "5471/5471 [==============================] - 436s 80ms/sample - loss: 0.3707\n",
      "Epoch 3/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3559\n",
      "validation rho: 0.3685\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_4_bert_ep3_fcv/assets\n",
      "5471/5471 [==============================] - 435s 79ms/sample - loss: 0.3558\n",
      "Epoch 4/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3411\n",
      "validation rho: 0.3733\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_4_bert_ep4_fcv/assets\n",
      "5471/5471 [==============================] - 434s 79ms/sample - loss: 0.3411\n",
      "Epoch 5/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3236\n",
      "validation rho: 0.3734\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_4_bert_ep5_fcv/assets\n",
      "5471/5471 [==============================] - 434s 79ms/sample - loss: 0.3236\n",
      "\n",
      "        FOLD 5\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3996\n",
      "validation rho: 0.3547\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_5_bert_ep1_fcv/assets\n",
      "5471/5471 [==============================] - 453s 83ms/sample - loss: 0.3997\n",
      "Epoch 2/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3698\n",
      "validation rho: 0.3705\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_5_bert_ep2_fcv/assets\n",
      "5471/5471 [==============================] - 436s 80ms/sample - loss: 0.3698\n",
      "Epoch 3/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3544\n",
      "validation rho: 0.3790\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_5_bert_ep3_fcv/assets\n",
      "5471/5471 [==============================] - 441s 81ms/sample - loss: 0.3544\n",
      "Epoch 4/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3395\n",
      "validation rho: 0.3816\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_5_bert_ep4_fcv/assets\n",
      "5471/5471 [==============================] - 435s 80ms/sample - loss: 0.3395\n",
      "Epoch 5/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3222\n",
      "validation rho: 0.3823\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_5_bert_ep5_fcv/assets\n",
      "5471/5471 [==============================] - 435s 79ms/sample - loss: 0.3222\n",
      "\n",
      "        FOLD 6\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3984\n",
      "validation rho: 0.3396\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_6_bert_ep1_fcv/assets\n",
      "5471/5471 [==============================] - 455s 83ms/sample - loss: 0.3984\n",
      "Epoch 2/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3686\n",
      "validation rho: 0.3566\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_6_bert_ep2_fcv/assets\n",
      "5471/5471 [==============================] - 436s 80ms/sample - loss: 0.3686\n",
      "Epoch 3/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3531\n",
      "validation rho: 0.3656\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_6_bert_ep3_fcv/assets\n",
      "5471/5471 [==============================] - 436s 80ms/sample - loss: 0.3531\n",
      "Epoch 4/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3369\n",
      "validation rho: 0.3701\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_6_bert_ep4_fcv/assets\n",
      "5471/5471 [==============================] - 441s 81ms/sample - loss: 0.3369\n",
      "Epoch 5/5\n",
      "5468/5471 [============================>.] - ETA: 0s - loss: 0.3178\n",
      "validation rho: 0.3692\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_6_bert_ep5_fcv/assets\n",
      "5471/5471 [==============================] - 435s 80ms/sample - loss: 0.3178\n",
      "\n",
      "        FOLD 7\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "   4/5471 [..............................] - ETA: 10:53:01\n",
      "validation rho: -0.0318\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_bert_ep_fcv/hist_7_bert_ep1_fcv/assets\n",
      "   4/5471 [..............................] - ETA: 25:29:22"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[4,512,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_model/StatefulPartitionedCall/encoder/layer_10/self_attention/query/add}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_728856]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a550c8b78711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                 \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                 \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                 loss_function='binary_crossentropy', fold=fold, times=fold+1)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#         histories.append(history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bf3df4003acf>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m(model, train_data, valid_data, test_data, learning_rate, epochs, batch_size, loss_function, fold, times)\u001b[0m\n\u001b[1;32m     16\u001b[0m     model.fit(train_data[0], train_data[1], epochs=epochs, \n\u001b[1;32m     17\u001b[0m               batch_size=batch_size, callbacks=[\n\u001b[0;32m---> 18\u001b[0;31m                   \u001b[0mcustom_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#                   save_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m               ])\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[4,512,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_model/StatefulPartitionedCall/encoder/layer_10/self_attention/query/add}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_728856]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "# histories = []\n",
    "models = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "    # will actually only do 3 folds (out of 5) to manage < 2h\n",
    "    if fold < 10:\n",
    "        K.clear_session()\n",
    "        \n",
    "#         strategy = tf.distribute.MirroredStrategy()\n",
    "#         with strategy.scope():\n",
    "        print('''\n",
    "        FOLD {}\n",
    "        '''.format(fold+1))\n",
    "        \n",
    "        model = bert_model()\n",
    "\n",
    "        train_inputs = [inputs[i][train_idx] for i in range(3)]\n",
    "        train_outputs = outputs[train_idx]\n",
    "\n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "\n",
    "            # history contains two lists of valid and test preds respectively:\n",
    "            #  [valid_predictions_{fold}, test_predictions_{fold}]\n",
    "\n",
    "\n",
    "        history, model = train_and_predict(model, \n",
    "                                train_data=(train_inputs, train_outputs), \n",
    "                                valid_data=(valid_inputs, valid_outputs),\n",
    "                                test_data=test_inputs, \n",
    "                                learning_rate=1e-5, epochs=5, batch_size=4,\n",
    "                                loss_function='binary_crossentropy', fold=fold, times=fold+1)\n",
    "\n",
    "#         histories.append(history)\n",
    "        models.append(model)\n",
    "        del history, model, train_inputs, train_outputs\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T20:05:02.433099Z",
     "start_time": "2020-01-18T16:22:33.020Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = load_model('../saved_models/hist_1/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
