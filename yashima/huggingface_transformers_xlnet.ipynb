{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:33.363290Z",
     "start_time": "2020-01-20T22:11:31.790712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../modules/transformers/')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:34.349601Z",
     "start_time": "2020-01-20T22:11:33.366267Z"
    }
   },
   "outputs": [],
   "source": [
    "# BERT_PATH = '../data/bert-base-uncased-huggingface-transformer/'\n",
    "# tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_sub = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some functions that will be used to preprocess the raw text data into useable Bert inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:34.360442Z",
     "start_time": "2020-01-20T22:11:34.353670Z"
    }
   },
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation_strategy=truncation_strategy)\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title + ' ' + question, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        answer, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :**  \n",
    "encode_plus 's return   \n",
    "{  \n",
    "    input_ids: list[int],  \n",
    "    token_type_ids: list[int] if return_token_type_ids is True (default)  \n",
    "    attention_mask: list[int] if return_attention_mask is True (default)  \n",
    "    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True  \n",
    "    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True  \n",
    "    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:34.382478Z",
     "start_time": "2020-01-20T22:11:34.363378Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:34.396671Z",
     "start_time": "2020-01-20T22:11:34.383883Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_spearmanr()` is used to compute the competition metric for the validation set,   \n",
    "`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:34.408229Z",
     "start_time": "2020-01-20T22:11:34.398172Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:34.428995Z",
     "start_time": "2020-01-20T22:11:34.409973Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "#     config =XLNetConfig() # print(config) to see settings\n",
    "    config =AlbertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "#     xlnet_model = TFAlbertModel.from_pretrained('albert-base-v1', config=config)\n",
    "    xlnet_model = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = xlnet_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = xlnet_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "\n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Obtain inputs and targets, as well as the indices of the train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:50.805880Z",
     "start_time": "2020-01-20T22:11:34.430951Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:14, 430.26it/s]\n",
      "476it [00:01, 450.42it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T22:11:50.811606Z",
     "start_time": "2020-01-20T22:11:50.807746Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, fold=None):\n",
    "        self.fold = fold\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         tf.saved_model.save(\n",
    "#             self.model, f'../saved_models/huggingface_albert/fold{fold+1}_ep{epoch+1}')\n",
    "        ckpt = tf.train.Checkpoint(model=self.model)\n",
    "        ckpt.save(f'../saved_models/huggingface_xlnet/fold{fold+1}_ep{epoch+1}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T23:15:49.361010Z",
     "start_time": "2020-01-20T22:11:50.812805Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        FOLD 1\n",
      "        \n",
      "WARNING:tensorflow:Entity <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4f342e8c90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4f342e8c90>>, which Python reported as:\n",
      "    def call(self, inputs, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
      "            token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, training=False):\n",
      "        if isinstance(inputs, (tuple, list)):\n",
      "            input_ids = inputs[0]\n",
      "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
      "            mems = inputs[2] if len(inputs) > 2 else mems\n",
      "            perm_mask = inputs[3] if len(inputs) > 3 else perm_mask\n",
      "            target_mapping = inputs[4] if len(inputs) > 4 else target_mapping\n",
      "            token_type_ids = inputs[5] if len(inputs) > 5 else token_type_ids\n",
      "            input_mask = inputs[6] if len(inputs) > 6 else input_mask\n",
      "            head_mask = inputs[7] if len(inputs) > 7 else head_mask\n",
      "            inputs_embeds = inputs[8] if len(inputs) > 8 else inputs_embeds\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        elif isinstance(inputs, dict):\n",
      "            input_ids = inputs.get('input_ids')\n",
      "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
      "            mems = inputs.get('mems', mems)\n",
      "            perm_mask = inputs.get('perm_mask', perm_mask)\n",
      "            target_mapping = inputs.get('target_mapping', target_mapping)\n",
      "            token_type_ids = inputs.get('token_type_ids', token_type_ids)\n",
      "            input_mask = inputs.get('input_mask', input_mask)\n",
      "            head_mask = inputs.get('head_mask', head_mask)\n",
      "            inputs_embeds = inputs.get('inputs_embeds', inputs_embeds)\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        else:\n",
      "            input_ids = inputs\n",
      "\n",
      "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
      "        # but we want a unified interface in the library with the batch size on the first dimension\n",
      "        # so we move here the first dimension (batch) to the end\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
      "            qlen, bsz = shape_list(input_ids)[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
      "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        token_type_ids = tf.transpose(token_type_ids, perm=(1, 0)) if token_type_ids is not None else None\n",
      "        input_mask = tf.transpose(input_mask, perm=(1, 0)) if input_mask is not None else None\n",
      "        attention_mask = tf.transpose(attention_mask, perm=(1, 0)) if attention_mask is not None else None\n",
      "        perm_mask = tf.transpose(perm_mask, perm=(1, 2, 0)) if perm_mask is not None else None\n",
      "        target_mapping = tf.transpose(target_mapping, perm=(1, 2, 0)) if target_mapping is not None else None\n",
      "\n",
      "        mlen = shape_list(mems[0])[0] if mems is not None and mems[0] is not None else 0\n",
      "        klen = mlen + qlen\n",
      "\n",
      "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
      "\n",
      "        ##### Attention mask\n",
      "        # causal attention mask\n",
      "        if self.attn_type == 'uni':\n",
      "            attn_mask = self.create_mask(qlen, mlen)\n",
      "            attn_mask = attn_mask[:, :, None, None]\n",
      "        elif self.attn_type == 'bi':\n",
      "            attn_mask = None\n",
      "        else:\n",
      "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
      "\n",
      "        # data mask: input mask & perm mask\n",
      "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n",
      "            \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
      "        if input_mask is None and attention_mask is not None:\n",
      "#             input_mask = 1.0 - attention_mask\n",
      "            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\n",
      "        if input_mask is not None and perm_mask is not None:\n",
      "            data_mask = input_mask[None] + perm_mask\n",
      "        elif input_mask is not None and perm_mask is None:\n",
      "            data_mask = input_mask[None]\n",
      "        elif input_mask is None and perm_mask is not None:\n",
      "            data_mask = perm_mask\n",
      "        else:\n",
      "            data_mask = None\n",
      "\n",
      "        if data_mask is not None:\n",
      "            # all mems can be attended to\n",
      "            mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz],\n",
      "                                dtype=dtype_float)\n",
      "            data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
      "            if attn_mask is None:\n",
      "                attn_mask = data_mask[:, :, :, None]\n",
      "            else:\n",
      "                attn_mask += data_mask[:, :, :, None]\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
      "            non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
      "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
      "        else:\n",
      "            non_tgt_mask = None\n",
      "\n",
      "        ##### Word embeddings and prepare h & g hidden states\n",
      "        if inputs_embeds is not None:\n",
      "            word_emb_k = inputs_embeds\n",
      "        else:\n",
      "            word_emb_k = self.word_embedding(input_ids)\n",
      "        output_h = self.dropout(word_emb_k, training=training)\n",
      "        if target_mapping is not None:\n",
      "            word_emb_q = tf.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])\n",
      "        # else:  # We removed the inp_q input which was same as target mapping\n",
      "        #     inp_q_ext = inp_q[:, :, None]\n",
      "        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
      "            output_g = self.dropout(word_emb_q, training=training)\n",
      "        else:\n",
      "            output_g = None\n",
      "\n",
      "        ##### Segment embedding\n",
      "        if token_type_ids is not None:\n",
      "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
      "            mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
      "            cat_ids = tf.concat([mem_pad, token_type_ids], 0)\n",
      "\n",
      "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
      "            seg_mat = tf.cast(\n",
      "                tf.logical_not(tf.equal(token_type_ids[:, None], cat_ids[None, :])),\n",
      "                tf.int32)\n",
      "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
      "        else:\n",
      "            seg_mat = None\n",
      "\n",
      "        ##### Positional encoding\n",
      "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
      "        pos_emb = self.dropout(pos_emb, training=training)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
      "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
      "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
      "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.n_layer\n",
      "\n",
      "        new_mems = ()\n",
      "        if mems is None:\n",
      "            mems = [None] * len(self.layer)\n",
      "\n",
      "        attentions = []\n",
      "        hidden_states = []\n",
      "        for i, layer_module in enumerate(self.layer):\n",
      "            # cache new mems\n",
      "            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
      "            if self.output_hidden_states:\n",
      "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "            outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n",
      "                                    pos_emb, seg_mat, mems[i], target_mapping,\n",
      "                                    head_mask[i]], training=training)\n",
      "            output_h, output_g = outputs[:2]\n",
      "            if self.output_attentions:\n",
      "                attentions.append(outputs[2])\n",
      "\n",
      "        # Add last hidden state\n",
      "        if self.output_hidden_states:\n",
      "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "        output = self.dropout(output_g if output_g is not None else output_h, training=training)\n",
      "\n",
      "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
      "        outputs = (tf.transpose(output, perm=(1, 0, 2)),)\n",
      "\n",
      "        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "            outputs = outputs + (new_mems,)\n",
      "\n",
      "        if self.output_hidden_states:\n",
      "            if output_g is not None:\n",
      "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
      "            else:\n",
      "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
      "            outputs = outputs + (hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
      "            outputs = outputs + (attentions,)\n",
      "\n",
      "        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4f342e8c90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4f342e8c90>>, which Python reported as:\n",
      "    def call(self, inputs, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
      "            token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, training=False):\n",
      "        if isinstance(inputs, (tuple, list)):\n",
      "            input_ids = inputs[0]\n",
      "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
      "            mems = inputs[2] if len(inputs) > 2 else mems\n",
      "            perm_mask = inputs[3] if len(inputs) > 3 else perm_mask\n",
      "            target_mapping = inputs[4] if len(inputs) > 4 else target_mapping\n",
      "            token_type_ids = inputs[5] if len(inputs) > 5 else token_type_ids\n",
      "            input_mask = inputs[6] if len(inputs) > 6 else input_mask\n",
      "            head_mask = inputs[7] if len(inputs) > 7 else head_mask\n",
      "            inputs_embeds = inputs[8] if len(inputs) > 8 else inputs_embeds\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        elif isinstance(inputs, dict):\n",
      "            input_ids = inputs.get('input_ids')\n",
      "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
      "            mems = inputs.get('mems', mems)\n",
      "            perm_mask = inputs.get('perm_mask', perm_mask)\n",
      "            target_mapping = inputs.get('target_mapping', target_mapping)\n",
      "            token_type_ids = inputs.get('token_type_ids', token_type_ids)\n",
      "            input_mask = inputs.get('input_mask', input_mask)\n",
      "            head_mask = inputs.get('head_mask', head_mask)\n",
      "            inputs_embeds = inputs.get('inputs_embeds', inputs_embeds)\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        else:\n",
      "            input_ids = inputs\n",
      "\n",
      "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
      "        # but we want a unified interface in the library with the batch size on the first dimension\n",
      "        # so we move here the first dimension (batch) to the end\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
      "            qlen, bsz = shape_list(input_ids)[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
      "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        token_type_ids = tf.transpose(token_type_ids, perm=(1, 0)) if token_type_ids is not None else None\n",
      "        input_mask = tf.transpose(input_mask, perm=(1, 0)) if input_mask is not None else None\n",
      "        attention_mask = tf.transpose(attention_mask, perm=(1, 0)) if attention_mask is not None else None\n",
      "        perm_mask = tf.transpose(perm_mask, perm=(1, 2, 0)) if perm_mask is not None else None\n",
      "        target_mapping = tf.transpose(target_mapping, perm=(1, 2, 0)) if target_mapping is not None else None\n",
      "\n",
      "        mlen = shape_list(mems[0])[0] if mems is not None and mems[0] is not None else 0\n",
      "        klen = mlen + qlen\n",
      "\n",
      "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
      "\n",
      "        ##### Attention mask\n",
      "        # causal attention mask\n",
      "        if self.attn_type == 'uni':\n",
      "            attn_mask = self.create_mask(qlen, mlen)\n",
      "            attn_mask = attn_mask[:, :, None, None]\n",
      "        elif self.attn_type == 'bi':\n",
      "            attn_mask = None\n",
      "        else:\n",
      "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
      "\n",
      "        # data mask: input mask & perm mask\n",
      "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n",
      "            \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
      "        if input_mask is None and attention_mask is not None:\n",
      "#             input_mask = 1.0 - attention_mask\n",
      "            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\n",
      "        if input_mask is not None and perm_mask is not None:\n",
      "            data_mask = input_mask[None] + perm_mask\n",
      "        elif input_mask is not None and perm_mask is None:\n",
      "            data_mask = input_mask[None]\n",
      "        elif input_mask is None and perm_mask is not None:\n",
      "            data_mask = perm_mask\n",
      "        else:\n",
      "            data_mask = None\n",
      "\n",
      "        if data_mask is not None:\n",
      "            # all mems can be attended to\n",
      "            mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz],\n",
      "                                dtype=dtype_float)\n",
      "            data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
      "            if attn_mask is None:\n",
      "                attn_mask = data_mask[:, :, :, None]\n",
      "            else:\n",
      "                attn_mask += data_mask[:, :, :, None]\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
      "            non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
      "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
      "        else:\n",
      "            non_tgt_mask = None\n",
      "\n",
      "        ##### Word embeddings and prepare h & g hidden states\n",
      "        if inputs_embeds is not None:\n",
      "            word_emb_k = inputs_embeds\n",
      "        else:\n",
      "            word_emb_k = self.word_embedding(input_ids)\n",
      "        output_h = self.dropout(word_emb_k, training=training)\n",
      "        if target_mapping is not None:\n",
      "            word_emb_q = tf.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])\n",
      "        # else:  # We removed the inp_q input which was same as target mapping\n",
      "        #     inp_q_ext = inp_q[:, :, None]\n",
      "        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
      "            output_g = self.dropout(word_emb_q, training=training)\n",
      "        else:\n",
      "            output_g = None\n",
      "\n",
      "        ##### Segment embedding\n",
      "        if token_type_ids is not None:\n",
      "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
      "            mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
      "            cat_ids = tf.concat([mem_pad, token_type_ids], 0)\n",
      "\n",
      "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
      "            seg_mat = tf.cast(\n",
      "                tf.logical_not(tf.equal(token_type_ids[:, None], cat_ids[None, :])),\n",
      "                tf.int32)\n",
      "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
      "        else:\n",
      "            seg_mat = None\n",
      "\n",
      "        ##### Positional encoding\n",
      "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
      "        pos_emb = self.dropout(pos_emb, training=training)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
      "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
      "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
      "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.n_layer\n",
      "\n",
      "        new_mems = ()\n",
      "        if mems is None:\n",
      "            mems = [None] * len(self.layer)\n",
      "\n",
      "        attentions = []\n",
      "        hidden_states = []\n",
      "        for i, layer_module in enumerate(self.layer):\n",
      "            # cache new mems\n",
      "            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
      "            if self.output_hidden_states:\n",
      "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "            outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n",
      "                                    pos_emb, seg_mat, mems[i], target_mapping,\n",
      "                                    head_mask[i]], training=training)\n",
      "            output_h, output_g = outputs[:2]\n",
      "            if self.output_attentions:\n",
      "                attentions.append(outputs[2])\n",
      "\n",
      "        # Add last hidden state\n",
      "        if self.output_hidden_states:\n",
      "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "        output = self.dropout(output_g if output_g is not None else output_h, training=training)\n",
      "\n",
      "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
      "        outputs = (tf.transpose(output, perm=(1, 0, 2)),)\n",
      "\n",
      "        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "            outputs = outputs + (new_mems,)\n",
      "\n",
      "        if self.output_hidden_states:\n",
      "            if output_g is not None:\n",
      "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
      "            else:\n",
      "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
      "            outputs = outputs + (hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
      "            outputs = outputs + (attentions,)\n",
      "\n",
      "        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4f342e8c90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4f342e8c90>>, which Python reported as:\n",
      "    def call(self, inputs, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
      "            token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, training=False):\n",
      "        if isinstance(inputs, (tuple, list)):\n",
      "            input_ids = inputs[0]\n",
      "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
      "            mems = inputs[2] if len(inputs) > 2 else mems\n",
      "            perm_mask = inputs[3] if len(inputs) > 3 else perm_mask\n",
      "            target_mapping = inputs[4] if len(inputs) > 4 else target_mapping\n",
      "            token_type_ids = inputs[5] if len(inputs) > 5 else token_type_ids\n",
      "            input_mask = inputs[6] if len(inputs) > 6 else input_mask\n",
      "            head_mask = inputs[7] if len(inputs) > 7 else head_mask\n",
      "            inputs_embeds = inputs[8] if len(inputs) > 8 else inputs_embeds\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        elif isinstance(inputs, dict):\n",
      "            input_ids = inputs.get('input_ids')\n",
      "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
      "            mems = inputs.get('mems', mems)\n",
      "            perm_mask = inputs.get('perm_mask', perm_mask)\n",
      "            target_mapping = inputs.get('target_mapping', target_mapping)\n",
      "            token_type_ids = inputs.get('token_type_ids', token_type_ids)\n",
      "            input_mask = inputs.get('input_mask', input_mask)\n",
      "            head_mask = inputs.get('head_mask', head_mask)\n",
      "            inputs_embeds = inputs.get('inputs_embeds', inputs_embeds)\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        else:\n",
      "            input_ids = inputs\n",
      "\n",
      "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
      "        # but we want a unified interface in the library with the batch size on the first dimension\n",
      "        # so we move here the first dimension (batch) to the end\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
      "            qlen, bsz = shape_list(input_ids)[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
      "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        token_type_ids = tf.transpose(token_type_ids, perm=(1, 0)) if token_type_ids is not None else None\n",
      "        input_mask = tf.transpose(input_mask, perm=(1, 0)) if input_mask is not None else None\n",
      "        attention_mask = tf.transpose(attention_mask, perm=(1, 0)) if attention_mask is not None else None\n",
      "        perm_mask = tf.transpose(perm_mask, perm=(1, 2, 0)) if perm_mask is not None else None\n",
      "        target_mapping = tf.transpose(target_mapping, perm=(1, 2, 0)) if target_mapping is not None else None\n",
      "\n",
      "        mlen = shape_list(mems[0])[0] if mems is not None and mems[0] is not None else 0\n",
      "        klen = mlen + qlen\n",
      "\n",
      "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
      "\n",
      "        ##### Attention mask\n",
      "        # causal attention mask\n",
      "        if self.attn_type == 'uni':\n",
      "            attn_mask = self.create_mask(qlen, mlen)\n",
      "            attn_mask = attn_mask[:, :, None, None]\n",
      "        elif self.attn_type == 'bi':\n",
      "            attn_mask = None\n",
      "        else:\n",
      "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
      "\n",
      "        # data mask: input mask & perm mask\n",
      "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n",
      "            \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
      "        if input_mask is None and attention_mask is not None:\n",
      "#             input_mask = 1.0 - attention_mask\n",
      "            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\n",
      "        if input_mask is not None and perm_mask is not None:\n",
      "            data_mask = input_mask[None] + perm_mask\n",
      "        elif input_mask is not None and perm_mask is None:\n",
      "            data_mask = input_mask[None]\n",
      "        elif input_mask is None and perm_mask is not None:\n",
      "            data_mask = perm_mask\n",
      "        else:\n",
      "            data_mask = None\n",
      "\n",
      "        if data_mask is not None:\n",
      "            # all mems can be attended to\n",
      "            mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz],\n",
      "                                dtype=dtype_float)\n",
      "            data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
      "            if attn_mask is None:\n",
      "                attn_mask = data_mask[:, :, :, None]\n",
      "            else:\n",
      "                attn_mask += data_mask[:, :, :, None]\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
      "            non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
      "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
      "        else:\n",
      "            non_tgt_mask = None\n",
      "\n",
      "        ##### Word embeddings and prepare h & g hidden states\n",
      "        if inputs_embeds is not None:\n",
      "            word_emb_k = inputs_embeds\n",
      "        else:\n",
      "            word_emb_k = self.word_embedding(input_ids)\n",
      "        output_h = self.dropout(word_emb_k, training=training)\n",
      "        if target_mapping is not None:\n",
      "            word_emb_q = tf.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])\n",
      "        # else:  # We removed the inp_q input which was same as target mapping\n",
      "        #     inp_q_ext = inp_q[:, :, None]\n",
      "        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
      "            output_g = self.dropout(word_emb_q, training=training)\n",
      "        else:\n",
      "            output_g = None\n",
      "\n",
      "        ##### Segment embedding\n",
      "        if token_type_ids is not None:\n",
      "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
      "            mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
      "            cat_ids = tf.concat([mem_pad, token_type_ids], 0)\n",
      "\n",
      "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
      "            seg_mat = tf.cast(\n",
      "                tf.logical_not(tf.equal(token_type_ids[:, None], cat_ids[None, :])),\n",
      "                tf.int32)\n",
      "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
      "        else:\n",
      "            seg_mat = None\n",
      "\n",
      "        ##### Positional encoding\n",
      "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
      "        pos_emb = self.dropout(pos_emb, training=training)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
      "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
      "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
      "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.n_layer\n",
      "\n",
      "        new_mems = ()\n",
      "        if mems is None:\n",
      "            mems = [None] * len(self.layer)\n",
      "\n",
      "        attentions = []\n",
      "        hidden_states = []\n",
      "        for i, layer_module in enumerate(self.layer):\n",
      "            # cache new mems\n",
      "            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
      "            if self.output_hidden_states:\n",
      "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "            outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n",
      "                                    pos_emb, seg_mat, mems[i], target_mapping,\n",
      "                                    head_mask[i]], training=training)\n",
      "            output_h, output_g = outputs[:2]\n",
      "            if self.output_attentions:\n",
      "                attentions.append(outputs[2])\n",
      "\n",
      "        # Add last hidden state\n",
      "        if self.output_hidden_states:\n",
      "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "        output = self.dropout(output_g if output_g is not None else output_h, training=training)\n",
      "\n",
      "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
      "        outputs = (tf.transpose(output, perm=(1, 0, 2)),)\n",
      "\n",
      "        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "            outputs = outputs + (new_mems,)\n",
      "\n",
      "        if self.output_hidden_states:\n",
      "            if output_g is not None:\n",
      "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
      "            else:\n",
      "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
      "            outputs = outputs + (hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
      "            outputs = outputs + (attentions,)\n",
      "\n",
      "        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4f342e8c90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4f342e8c90>>, which Python reported as:\n",
      "    def call(self, inputs, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
      "            token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, training=False):\n",
      "        if isinstance(inputs, (tuple, list)):\n",
      "            input_ids = inputs[0]\n",
      "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
      "            mems = inputs[2] if len(inputs) > 2 else mems\n",
      "            perm_mask = inputs[3] if len(inputs) > 3 else perm_mask\n",
      "            target_mapping = inputs[4] if len(inputs) > 4 else target_mapping\n",
      "            token_type_ids = inputs[5] if len(inputs) > 5 else token_type_ids\n",
      "            input_mask = inputs[6] if len(inputs) > 6 else input_mask\n",
      "            head_mask = inputs[7] if len(inputs) > 7 else head_mask\n",
      "            inputs_embeds = inputs[8] if len(inputs) > 8 else inputs_embeds\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        elif isinstance(inputs, dict):\n",
      "            input_ids = inputs.get('input_ids')\n",
      "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
      "            mems = inputs.get('mems', mems)\n",
      "            perm_mask = inputs.get('perm_mask', perm_mask)\n",
      "            target_mapping = inputs.get('target_mapping', target_mapping)\n",
      "            token_type_ids = inputs.get('token_type_ids', token_type_ids)\n",
      "            input_mask = inputs.get('input_mask', input_mask)\n",
      "            head_mask = inputs.get('head_mask', head_mask)\n",
      "            inputs_embeds = inputs.get('inputs_embeds', inputs_embeds)\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        else:\n",
      "            input_ids = inputs\n",
      "\n",
      "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
      "        # but we want a unified interface in the library with the batch size on the first dimension\n",
      "        # so we move here the first dimension (batch) to the end\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
      "            qlen, bsz = shape_list(input_ids)[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
      "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        token_type_ids = tf.transpose(token_type_ids, perm=(1, 0)) if token_type_ids is not None else None\n",
      "        input_mask = tf.transpose(input_mask, perm=(1, 0)) if input_mask is not None else None\n",
      "        attention_mask = tf.transpose(attention_mask, perm=(1, 0)) if attention_mask is not None else None\n",
      "        perm_mask = tf.transpose(perm_mask, perm=(1, 2, 0)) if perm_mask is not None else None\n",
      "        target_mapping = tf.transpose(target_mapping, perm=(1, 2, 0)) if target_mapping is not None else None\n",
      "\n",
      "        mlen = shape_list(mems[0])[0] if mems is not None and mems[0] is not None else 0\n",
      "        klen = mlen + qlen\n",
      "\n",
      "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
      "\n",
      "        ##### Attention mask\n",
      "        # causal attention mask\n",
      "        if self.attn_type == 'uni':\n",
      "            attn_mask = self.create_mask(qlen, mlen)\n",
      "            attn_mask = attn_mask[:, :, None, None]\n",
      "        elif self.attn_type == 'bi':\n",
      "            attn_mask = None\n",
      "        else:\n",
      "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
      "\n",
      "        # data mask: input mask & perm mask\n",
      "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n",
      "            \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
      "        if input_mask is None and attention_mask is not None:\n",
      "#             input_mask = 1.0 - attention_mask\n",
      "            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\n",
      "        if input_mask is not None and perm_mask is not None:\n",
      "            data_mask = input_mask[None] + perm_mask\n",
      "        elif input_mask is not None and perm_mask is None:\n",
      "            data_mask = input_mask[None]\n",
      "        elif input_mask is None and perm_mask is not None:\n",
      "            data_mask = perm_mask\n",
      "        else:\n",
      "            data_mask = None\n",
      "\n",
      "        if data_mask is not None:\n",
      "            # all mems can be attended to\n",
      "            mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz],\n",
      "                                dtype=dtype_float)\n",
      "            data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
      "            if attn_mask is None:\n",
      "                attn_mask = data_mask[:, :, :, None]\n",
      "            else:\n",
      "                attn_mask += data_mask[:, :, :, None]\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
      "            non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
      "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
      "        else:\n",
      "            non_tgt_mask = None\n",
      "\n",
      "        ##### Word embeddings and prepare h & g hidden states\n",
      "        if inputs_embeds is not None:\n",
      "            word_emb_k = inputs_embeds\n",
      "        else:\n",
      "            word_emb_k = self.word_embedding(input_ids)\n",
      "        output_h = self.dropout(word_emb_k, training=training)\n",
      "        if target_mapping is not None:\n",
      "            word_emb_q = tf.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])\n",
      "        # else:  # We removed the inp_q input which was same as target mapping\n",
      "        #     inp_q_ext = inp_q[:, :, None]\n",
      "        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
      "            output_g = self.dropout(word_emb_q, training=training)\n",
      "        else:\n",
      "            output_g = None\n",
      "\n",
      "        ##### Segment embedding\n",
      "        if token_type_ids is not None:\n",
      "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
      "            mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
      "            cat_ids = tf.concat([mem_pad, token_type_ids], 0)\n",
      "\n",
      "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
      "            seg_mat = tf.cast(\n",
      "                tf.logical_not(tf.equal(token_type_ids[:, None], cat_ids[None, :])),\n",
      "                tf.int32)\n",
      "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
      "        else:\n",
      "            seg_mat = None\n",
      "\n",
      "        ##### Positional encoding\n",
      "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
      "        pos_emb = self.dropout(pos_emb, training=training)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
      "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
      "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
      "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.n_layer\n",
      "\n",
      "        new_mems = ()\n",
      "        if mems is None:\n",
      "            mems = [None] * len(self.layer)\n",
      "\n",
      "        attentions = []\n",
      "        hidden_states = []\n",
      "        for i, layer_module in enumerate(self.layer):\n",
      "            # cache new mems\n",
      "            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
      "            if self.output_hidden_states:\n",
      "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "            outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n",
      "                                    pos_emb, seg_mat, mems[i], target_mapping,\n",
      "                                    head_mask[i]], training=training)\n",
      "            output_h, output_g = outputs[:2]\n",
      "            if self.output_attentions:\n",
      "                attentions.append(outputs[2])\n",
      "\n",
      "        # Add last hidden state\n",
      "        if self.output_hidden_states:\n",
      "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "        output = self.dropout(output_g if output_g is not None else output_h, training=training)\n",
      "\n",
      "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
      "        outputs = (tf.transpose(output, perm=(1, 0, 2)),)\n",
      "\n",
      "        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "            outputs = outputs + (new_mems,)\n",
      "\n",
      "        if self.output_hidden_states:\n",
      "            if output_g is not None:\n",
      "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
      "            else:\n",
      "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
      "            outputs = outputs + (hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
      "            outputs = outputs + (attentions,)\n",
      "\n",
      "        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4863 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 206 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 206 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "4864/4863 [==============================] - 717s 147ms/sample - loss: 0.4030\n",
      "Epoch 2/3\n",
      "4864/4863 [==============================] - 536s 110ms/sample - loss: 0.3696\n",
      "Epoch 3/3\n",
      "4864/4863 [==============================] - 535s 110ms/sample - loss: 0.3583\n",
      "validation score =  0.4002303231930225\n",
      "\n",
      "        FOLD 3\n",
      "        \n",
      "WARNING:tensorflow:Entity <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4181258ed0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4181258ed0>>, which Python reported as:\n",
      "    def call(self, inputs, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
      "            token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, training=False):\n",
      "        if isinstance(inputs, (tuple, list)):\n",
      "            input_ids = inputs[0]\n",
      "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
      "            mems = inputs[2] if len(inputs) > 2 else mems\n",
      "            perm_mask = inputs[3] if len(inputs) > 3 else perm_mask\n",
      "            target_mapping = inputs[4] if len(inputs) > 4 else target_mapping\n",
      "            token_type_ids = inputs[5] if len(inputs) > 5 else token_type_ids\n",
      "            input_mask = inputs[6] if len(inputs) > 6 else input_mask\n",
      "            head_mask = inputs[7] if len(inputs) > 7 else head_mask\n",
      "            inputs_embeds = inputs[8] if len(inputs) > 8 else inputs_embeds\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        elif isinstance(inputs, dict):\n",
      "            input_ids = inputs.get('input_ids')\n",
      "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
      "            mems = inputs.get('mems', mems)\n",
      "            perm_mask = inputs.get('perm_mask', perm_mask)\n",
      "            target_mapping = inputs.get('target_mapping', target_mapping)\n",
      "            token_type_ids = inputs.get('token_type_ids', token_type_ids)\n",
      "            input_mask = inputs.get('input_mask', input_mask)\n",
      "            head_mask = inputs.get('head_mask', head_mask)\n",
      "            inputs_embeds = inputs.get('inputs_embeds', inputs_embeds)\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        else:\n",
      "            input_ids = inputs\n",
      "\n",
      "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
      "        # but we want a unified interface in the library with the batch size on the first dimension\n",
      "        # so we move here the first dimension (batch) to the end\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
      "            qlen, bsz = shape_list(input_ids)[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
      "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        token_type_ids = tf.transpose(token_type_ids, perm=(1, 0)) if token_type_ids is not None else None\n",
      "        input_mask = tf.transpose(input_mask, perm=(1, 0)) if input_mask is not None else None\n",
      "        attention_mask = tf.transpose(attention_mask, perm=(1, 0)) if attention_mask is not None else None\n",
      "        perm_mask = tf.transpose(perm_mask, perm=(1, 2, 0)) if perm_mask is not None else None\n",
      "        target_mapping = tf.transpose(target_mapping, perm=(1, 2, 0)) if target_mapping is not None else None\n",
      "\n",
      "        mlen = shape_list(mems[0])[0] if mems is not None and mems[0] is not None else 0\n",
      "        klen = mlen + qlen\n",
      "\n",
      "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
      "\n",
      "        ##### Attention mask\n",
      "        # causal attention mask\n",
      "        if self.attn_type == 'uni':\n",
      "            attn_mask = self.create_mask(qlen, mlen)\n",
      "            attn_mask = attn_mask[:, :, None, None]\n",
      "        elif self.attn_type == 'bi':\n",
      "            attn_mask = None\n",
      "        else:\n",
      "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
      "\n",
      "        # data mask: input mask & perm mask\n",
      "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n",
      "            \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
      "        if input_mask is None and attention_mask is not None:\n",
      "#             input_mask = 1.0 - attention_mask\n",
      "            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\n",
      "        if input_mask is not None and perm_mask is not None:\n",
      "            data_mask = input_mask[None] + perm_mask\n",
      "        elif input_mask is not None and perm_mask is None:\n",
      "            data_mask = input_mask[None]\n",
      "        elif input_mask is None and perm_mask is not None:\n",
      "            data_mask = perm_mask\n",
      "        else:\n",
      "            data_mask = None\n",
      "\n",
      "        if data_mask is not None:\n",
      "            # all mems can be attended to\n",
      "            mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz],\n",
      "                                dtype=dtype_float)\n",
      "            data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
      "            if attn_mask is None:\n",
      "                attn_mask = data_mask[:, :, :, None]\n",
      "            else:\n",
      "                attn_mask += data_mask[:, :, :, None]\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
      "            non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
      "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
      "        else:\n",
      "            non_tgt_mask = None\n",
      "\n",
      "        ##### Word embeddings and prepare h & g hidden states\n",
      "        if inputs_embeds is not None:\n",
      "            word_emb_k = inputs_embeds\n",
      "        else:\n",
      "            word_emb_k = self.word_embedding(input_ids)\n",
      "        output_h = self.dropout(word_emb_k, training=training)\n",
      "        if target_mapping is not None:\n",
      "            word_emb_q = tf.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])\n",
      "        # else:  # We removed the inp_q input which was same as target mapping\n",
      "        #     inp_q_ext = inp_q[:, :, None]\n",
      "        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
      "            output_g = self.dropout(word_emb_q, training=training)\n",
      "        else:\n",
      "            output_g = None\n",
      "\n",
      "        ##### Segment embedding\n",
      "        if token_type_ids is not None:\n",
      "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
      "            mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
      "            cat_ids = tf.concat([mem_pad, token_type_ids], 0)\n",
      "\n",
      "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
      "            seg_mat = tf.cast(\n",
      "                tf.logical_not(tf.equal(token_type_ids[:, None], cat_ids[None, :])),\n",
      "                tf.int32)\n",
      "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
      "        else:\n",
      "            seg_mat = None\n",
      "\n",
      "        ##### Positional encoding\n",
      "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
      "        pos_emb = self.dropout(pos_emb, training=training)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
      "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
      "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
      "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.n_layer\n",
      "\n",
      "        new_mems = ()\n",
      "        if mems is None:\n",
      "            mems = [None] * len(self.layer)\n",
      "\n",
      "        attentions = []\n",
      "        hidden_states = []\n",
      "        for i, layer_module in enumerate(self.layer):\n",
      "            # cache new mems\n",
      "            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
      "            if self.output_hidden_states:\n",
      "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "            outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n",
      "                                    pos_emb, seg_mat, mems[i], target_mapping,\n",
      "                                    head_mask[i]], training=training)\n",
      "            output_h, output_g = outputs[:2]\n",
      "            if self.output_attentions:\n",
      "                attentions.append(outputs[2])\n",
      "\n",
      "        # Add last hidden state\n",
      "        if self.output_hidden_states:\n",
      "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "        output = self.dropout(output_g if output_g is not None else output_h, training=training)\n",
      "\n",
      "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
      "        outputs = (tf.transpose(output, perm=(1, 0, 2)),)\n",
      "\n",
      "        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "            outputs = outputs + (new_mems,)\n",
      "\n",
      "        if self.output_hidden_states:\n",
      "            if output_g is not None:\n",
      "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
      "            else:\n",
      "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
      "            outputs = outputs + (hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
      "            outputs = outputs + (attentions,)\n",
      "\n",
      "        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4181258ed0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4181258ed0>>, which Python reported as:\n",
      "    def call(self, inputs, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
      "            token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, training=False):\n",
      "        if isinstance(inputs, (tuple, list)):\n",
      "            input_ids = inputs[0]\n",
      "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
      "            mems = inputs[2] if len(inputs) > 2 else mems\n",
      "            perm_mask = inputs[3] if len(inputs) > 3 else perm_mask\n",
      "            target_mapping = inputs[4] if len(inputs) > 4 else target_mapping\n",
      "            token_type_ids = inputs[5] if len(inputs) > 5 else token_type_ids\n",
      "            input_mask = inputs[6] if len(inputs) > 6 else input_mask\n",
      "            head_mask = inputs[7] if len(inputs) > 7 else head_mask\n",
      "            inputs_embeds = inputs[8] if len(inputs) > 8 else inputs_embeds\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        elif isinstance(inputs, dict):\n",
      "            input_ids = inputs.get('input_ids')\n",
      "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
      "            mems = inputs.get('mems', mems)\n",
      "            perm_mask = inputs.get('perm_mask', perm_mask)\n",
      "            target_mapping = inputs.get('target_mapping', target_mapping)\n",
      "            token_type_ids = inputs.get('token_type_ids', token_type_ids)\n",
      "            input_mask = inputs.get('input_mask', input_mask)\n",
      "            head_mask = inputs.get('head_mask', head_mask)\n",
      "            inputs_embeds = inputs.get('inputs_embeds', inputs_embeds)\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        else:\n",
      "            input_ids = inputs\n",
      "\n",
      "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
      "        # but we want a unified interface in the library with the batch size on the first dimension\n",
      "        # so we move here the first dimension (batch) to the end\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
      "            qlen, bsz = shape_list(input_ids)[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
      "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        token_type_ids = tf.transpose(token_type_ids, perm=(1, 0)) if token_type_ids is not None else None\n",
      "        input_mask = tf.transpose(input_mask, perm=(1, 0)) if input_mask is not None else None\n",
      "        attention_mask = tf.transpose(attention_mask, perm=(1, 0)) if attention_mask is not None else None\n",
      "        perm_mask = tf.transpose(perm_mask, perm=(1, 2, 0)) if perm_mask is not None else None\n",
      "        target_mapping = tf.transpose(target_mapping, perm=(1, 2, 0)) if target_mapping is not None else None\n",
      "\n",
      "        mlen = shape_list(mems[0])[0] if mems is not None and mems[0] is not None else 0\n",
      "        klen = mlen + qlen\n",
      "\n",
      "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
      "\n",
      "        ##### Attention mask\n",
      "        # causal attention mask\n",
      "        if self.attn_type == 'uni':\n",
      "            attn_mask = self.create_mask(qlen, mlen)\n",
      "            attn_mask = attn_mask[:, :, None, None]\n",
      "        elif self.attn_type == 'bi':\n",
      "            attn_mask = None\n",
      "        else:\n",
      "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
      "\n",
      "        # data mask: input mask & perm mask\n",
      "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n",
      "            \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
      "        if input_mask is None and attention_mask is not None:\n",
      "#             input_mask = 1.0 - attention_mask\n",
      "            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\n",
      "        if input_mask is not None and perm_mask is not None:\n",
      "            data_mask = input_mask[None] + perm_mask\n",
      "        elif input_mask is not None and perm_mask is None:\n",
      "            data_mask = input_mask[None]\n",
      "        elif input_mask is None and perm_mask is not None:\n",
      "            data_mask = perm_mask\n",
      "        else:\n",
      "            data_mask = None\n",
      "\n",
      "        if data_mask is not None:\n",
      "            # all mems can be attended to\n",
      "            mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz],\n",
      "                                dtype=dtype_float)\n",
      "            data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
      "            if attn_mask is None:\n",
      "                attn_mask = data_mask[:, :, :, None]\n",
      "            else:\n",
      "                attn_mask += data_mask[:, :, :, None]\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
      "            non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
      "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
      "        else:\n",
      "            non_tgt_mask = None\n",
      "\n",
      "        ##### Word embeddings and prepare h & g hidden states\n",
      "        if inputs_embeds is not None:\n",
      "            word_emb_k = inputs_embeds\n",
      "        else:\n",
      "            word_emb_k = self.word_embedding(input_ids)\n",
      "        output_h = self.dropout(word_emb_k, training=training)\n",
      "        if target_mapping is not None:\n",
      "            word_emb_q = tf.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])\n",
      "        # else:  # We removed the inp_q input which was same as target mapping\n",
      "        #     inp_q_ext = inp_q[:, :, None]\n",
      "        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
      "            output_g = self.dropout(word_emb_q, training=training)\n",
      "        else:\n",
      "            output_g = None\n",
      "\n",
      "        ##### Segment embedding\n",
      "        if token_type_ids is not None:\n",
      "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
      "            mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
      "            cat_ids = tf.concat([mem_pad, token_type_ids], 0)\n",
      "\n",
      "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
      "            seg_mat = tf.cast(\n",
      "                tf.logical_not(tf.equal(token_type_ids[:, None], cat_ids[None, :])),\n",
      "                tf.int32)\n",
      "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
      "        else:\n",
      "            seg_mat = None\n",
      "\n",
      "        ##### Positional encoding\n",
      "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
      "        pos_emb = self.dropout(pos_emb, training=training)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
      "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
      "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
      "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.n_layer\n",
      "\n",
      "        new_mems = ()\n",
      "        if mems is None:\n",
      "            mems = [None] * len(self.layer)\n",
      "\n",
      "        attentions = []\n",
      "        hidden_states = []\n",
      "        for i, layer_module in enumerate(self.layer):\n",
      "            # cache new mems\n",
      "            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
      "            if self.output_hidden_states:\n",
      "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "            outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n",
      "                                    pos_emb, seg_mat, mems[i], target_mapping,\n",
      "                                    head_mask[i]], training=training)\n",
      "            output_h, output_g = outputs[:2]\n",
      "            if self.output_attentions:\n",
      "                attentions.append(outputs[2])\n",
      "\n",
      "        # Add last hidden state\n",
      "        if self.output_hidden_states:\n",
      "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "        output = self.dropout(output_g if output_g is not None else output_h, training=training)\n",
      "\n",
      "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
      "        outputs = (tf.transpose(output, perm=(1, 0, 2)),)\n",
      "\n",
      "        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "            outputs = outputs + (new_mems,)\n",
      "\n",
      "        if self.output_hidden_states:\n",
      "            if output_g is not None:\n",
      "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
      "            else:\n",
      "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
      "            outputs = outputs + (hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
      "            outputs = outputs + (attentions,)\n",
      "\n",
      "        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4181258ed0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4181258ed0>>, which Python reported as:\n",
      "    def call(self, inputs, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
      "            token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, training=False):\n",
      "        if isinstance(inputs, (tuple, list)):\n",
      "            input_ids = inputs[0]\n",
      "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
      "            mems = inputs[2] if len(inputs) > 2 else mems\n",
      "            perm_mask = inputs[3] if len(inputs) > 3 else perm_mask\n",
      "            target_mapping = inputs[4] if len(inputs) > 4 else target_mapping\n",
      "            token_type_ids = inputs[5] if len(inputs) > 5 else token_type_ids\n",
      "            input_mask = inputs[6] if len(inputs) > 6 else input_mask\n",
      "            head_mask = inputs[7] if len(inputs) > 7 else head_mask\n",
      "            inputs_embeds = inputs[8] if len(inputs) > 8 else inputs_embeds\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        elif isinstance(inputs, dict):\n",
      "            input_ids = inputs.get('input_ids')\n",
      "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
      "            mems = inputs.get('mems', mems)\n",
      "            perm_mask = inputs.get('perm_mask', perm_mask)\n",
      "            target_mapping = inputs.get('target_mapping', target_mapping)\n",
      "            token_type_ids = inputs.get('token_type_ids', token_type_ids)\n",
      "            input_mask = inputs.get('input_mask', input_mask)\n",
      "            head_mask = inputs.get('head_mask', head_mask)\n",
      "            inputs_embeds = inputs.get('inputs_embeds', inputs_embeds)\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        else:\n",
      "            input_ids = inputs\n",
      "\n",
      "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
      "        # but we want a unified interface in the library with the batch size on the first dimension\n",
      "        # so we move here the first dimension (batch) to the end\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
      "            qlen, bsz = shape_list(input_ids)[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
      "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        token_type_ids = tf.transpose(token_type_ids, perm=(1, 0)) if token_type_ids is not None else None\n",
      "        input_mask = tf.transpose(input_mask, perm=(1, 0)) if input_mask is not None else None\n",
      "        attention_mask = tf.transpose(attention_mask, perm=(1, 0)) if attention_mask is not None else None\n",
      "        perm_mask = tf.transpose(perm_mask, perm=(1, 2, 0)) if perm_mask is not None else None\n",
      "        target_mapping = tf.transpose(target_mapping, perm=(1, 2, 0)) if target_mapping is not None else None\n",
      "\n",
      "        mlen = shape_list(mems[0])[0] if mems is not None and mems[0] is not None else 0\n",
      "        klen = mlen + qlen\n",
      "\n",
      "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
      "\n",
      "        ##### Attention mask\n",
      "        # causal attention mask\n",
      "        if self.attn_type == 'uni':\n",
      "            attn_mask = self.create_mask(qlen, mlen)\n",
      "            attn_mask = attn_mask[:, :, None, None]\n",
      "        elif self.attn_type == 'bi':\n",
      "            attn_mask = None\n",
      "        else:\n",
      "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
      "\n",
      "        # data mask: input mask & perm mask\n",
      "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n",
      "            \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
      "        if input_mask is None and attention_mask is not None:\n",
      "#             input_mask = 1.0 - attention_mask\n",
      "            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\n",
      "        if input_mask is not None and perm_mask is not None:\n",
      "            data_mask = input_mask[None] + perm_mask\n",
      "        elif input_mask is not None and perm_mask is None:\n",
      "            data_mask = input_mask[None]\n",
      "        elif input_mask is None and perm_mask is not None:\n",
      "            data_mask = perm_mask\n",
      "        else:\n",
      "            data_mask = None\n",
      "\n",
      "        if data_mask is not None:\n",
      "            # all mems can be attended to\n",
      "            mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz],\n",
      "                                dtype=dtype_float)\n",
      "            data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
      "            if attn_mask is None:\n",
      "                attn_mask = data_mask[:, :, :, None]\n",
      "            else:\n",
      "                attn_mask += data_mask[:, :, :, None]\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
      "            non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
      "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
      "        else:\n",
      "            non_tgt_mask = None\n",
      "\n",
      "        ##### Word embeddings and prepare h & g hidden states\n",
      "        if inputs_embeds is not None:\n",
      "            word_emb_k = inputs_embeds\n",
      "        else:\n",
      "            word_emb_k = self.word_embedding(input_ids)\n",
      "        output_h = self.dropout(word_emb_k, training=training)\n",
      "        if target_mapping is not None:\n",
      "            word_emb_q = tf.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])\n",
      "        # else:  # We removed the inp_q input which was same as target mapping\n",
      "        #     inp_q_ext = inp_q[:, :, None]\n",
      "        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
      "            output_g = self.dropout(word_emb_q, training=training)\n",
      "        else:\n",
      "            output_g = None\n",
      "\n",
      "        ##### Segment embedding\n",
      "        if token_type_ids is not None:\n",
      "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
      "            mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
      "            cat_ids = tf.concat([mem_pad, token_type_ids], 0)\n",
      "\n",
      "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
      "            seg_mat = tf.cast(\n",
      "                tf.logical_not(tf.equal(token_type_ids[:, None], cat_ids[None, :])),\n",
      "                tf.int32)\n",
      "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
      "        else:\n",
      "            seg_mat = None\n",
      "\n",
      "        ##### Positional encoding\n",
      "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
      "        pos_emb = self.dropout(pos_emb, training=training)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
      "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
      "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
      "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.n_layer\n",
      "\n",
      "        new_mems = ()\n",
      "        if mems is None:\n",
      "            mems = [None] * len(self.layer)\n",
      "\n",
      "        attentions = []\n",
      "        hidden_states = []\n",
      "        for i, layer_module in enumerate(self.layer):\n",
      "            # cache new mems\n",
      "            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
      "            if self.output_hidden_states:\n",
      "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "            outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n",
      "                                    pos_emb, seg_mat, mems[i], target_mapping,\n",
      "                                    head_mask[i]], training=training)\n",
      "            output_h, output_g = outputs[:2]\n",
      "            if self.output_attentions:\n",
      "                attentions.append(outputs[2])\n",
      "\n",
      "        # Add last hidden state\n",
      "        if self.output_hidden_states:\n",
      "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "        output = self.dropout(output_g if output_g is not None else output_h, training=training)\n",
      "\n",
      "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
      "        outputs = (tf.transpose(output, perm=(1, 0, 2)),)\n",
      "\n",
      "        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "            outputs = outputs + (new_mems,)\n",
      "\n",
      "        if self.output_hidden_states:\n",
      "            if output_g is not None:\n",
      "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
      "            else:\n",
      "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
      "            outputs = outputs + (hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
      "            outputs = outputs + (attentions,)\n",
      "\n",
      "        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4181258ed0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TFXLNetMainLayer.call of <transformers.modeling_tf_xlnet.TFXLNetMainLayer object at 0x7f4181258ed0>>, which Python reported as:\n",
      "    def call(self, inputs, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
      "            token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, training=False):\n",
      "        if isinstance(inputs, (tuple, list)):\n",
      "            input_ids = inputs[0]\n",
      "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
      "            mems = inputs[2] if len(inputs) > 2 else mems\n",
      "            perm_mask = inputs[3] if len(inputs) > 3 else perm_mask\n",
      "            target_mapping = inputs[4] if len(inputs) > 4 else target_mapping\n",
      "            token_type_ids = inputs[5] if len(inputs) > 5 else token_type_ids\n",
      "            input_mask = inputs[6] if len(inputs) > 6 else input_mask\n",
      "            head_mask = inputs[7] if len(inputs) > 7 else head_mask\n",
      "            inputs_embeds = inputs[8] if len(inputs) > 8 else inputs_embeds\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        elif isinstance(inputs, dict):\n",
      "            input_ids = inputs.get('input_ids')\n",
      "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
      "            mems = inputs.get('mems', mems)\n",
      "            perm_mask = inputs.get('perm_mask', perm_mask)\n",
      "            target_mapping = inputs.get('target_mapping', target_mapping)\n",
      "            token_type_ids = inputs.get('token_type_ids', token_type_ids)\n",
      "            input_mask = inputs.get('input_mask', input_mask)\n",
      "            head_mask = inputs.get('head_mask', head_mask)\n",
      "            inputs_embeds = inputs.get('inputs_embeds', inputs_embeds)\n",
      "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
      "        else:\n",
      "            input_ids = inputs\n",
      "\n",
      "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
      "        # but we want a unified interface in the library with the batch size on the first dimension\n",
      "        # so we move here the first dimension (batch) to the end\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
      "            qlen, bsz = shape_list(input_ids)[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
      "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        token_type_ids = tf.transpose(token_type_ids, perm=(1, 0)) if token_type_ids is not None else None\n",
      "        input_mask = tf.transpose(input_mask, perm=(1, 0)) if input_mask is not None else None\n",
      "        attention_mask = tf.transpose(attention_mask, perm=(1, 0)) if attention_mask is not None else None\n",
      "        perm_mask = tf.transpose(perm_mask, perm=(1, 2, 0)) if perm_mask is not None else None\n",
      "        target_mapping = tf.transpose(target_mapping, perm=(1, 2, 0)) if target_mapping is not None else None\n",
      "\n",
      "        mlen = shape_list(mems[0])[0] if mems is not None and mems[0] is not None else 0\n",
      "        klen = mlen + qlen\n",
      "\n",
      "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
      "\n",
      "        ##### Attention mask\n",
      "        # causal attention mask\n",
      "        if self.attn_type == 'uni':\n",
      "            attn_mask = self.create_mask(qlen, mlen)\n",
      "            attn_mask = attn_mask[:, :, None, None]\n",
      "        elif self.attn_type == 'bi':\n",
      "            attn_mask = None\n",
      "        else:\n",
      "            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n",
      "\n",
      "        # data mask: input mask & perm mask\n",
      "        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n",
      "            \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n",
      "        if input_mask is None and attention_mask is not None:\n",
      "#             input_mask = 1.0 - attention_mask\n",
      "            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\n",
      "        if input_mask is not None and perm_mask is not None:\n",
      "            data_mask = input_mask[None] + perm_mask\n",
      "        elif input_mask is not None and perm_mask is None:\n",
      "            data_mask = input_mask[None]\n",
      "        elif input_mask is None and perm_mask is not None:\n",
      "            data_mask = perm_mask\n",
      "        else:\n",
      "            data_mask = None\n",
      "\n",
      "        if data_mask is not None:\n",
      "            # all mems can be attended to\n",
      "            mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz],\n",
      "                                dtype=dtype_float)\n",
      "            data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
      "            if attn_mask is None:\n",
      "                attn_mask = data_mask[:, :, :, None]\n",
      "            else:\n",
      "                attn_mask += data_mask[:, :, :, None]\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
      "            non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
      "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
      "        else:\n",
      "            non_tgt_mask = None\n",
      "\n",
      "        ##### Word embeddings and prepare h & g hidden states\n",
      "        if inputs_embeds is not None:\n",
      "            word_emb_k = inputs_embeds\n",
      "        else:\n",
      "            word_emb_k = self.word_embedding(input_ids)\n",
      "        output_h = self.dropout(word_emb_k, training=training)\n",
      "        if target_mapping is not None:\n",
      "            word_emb_q = tf.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])\n",
      "        # else:  # We removed the inp_q input which was same as target mapping\n",
      "        #     inp_q_ext = inp_q[:, :, None]\n",
      "        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
      "            output_g = self.dropout(word_emb_q, training=training)\n",
      "        else:\n",
      "            output_g = None\n",
      "\n",
      "        ##### Segment embedding\n",
      "        if token_type_ids is not None:\n",
      "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
      "            mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
      "            cat_ids = tf.concat([mem_pad, token_type_ids], 0)\n",
      "\n",
      "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
      "            seg_mat = tf.cast(\n",
      "                tf.logical_not(tf.equal(token_type_ids[:, None], cat_ids[None, :])),\n",
      "                tf.int32)\n",
      "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
      "        else:\n",
      "            seg_mat = None\n",
      "\n",
      "        ##### Positional encoding\n",
      "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
      "        pos_emb = self.dropout(pos_emb, training=training)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
      "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
      "                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
      "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.n_layer\n",
      "\n",
      "        new_mems = ()\n",
      "        if mems is None:\n",
      "            mems = [None] * len(self.layer)\n",
      "\n",
      "        attentions = []\n",
      "        hidden_states = []\n",
      "        for i, layer_module in enumerate(self.layer):\n",
      "            # cache new mems\n",
      "            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n",
      "            if self.output_hidden_states:\n",
      "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "            outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n",
      "                                    pos_emb, seg_mat, mems[i], target_mapping,\n",
      "                                    head_mask[i]], training=training)\n",
      "            output_h, output_g = outputs[:2]\n",
      "            if self.output_attentions:\n",
      "                attentions.append(outputs[2])\n",
      "\n",
      "        # Add last hidden state\n",
      "        if self.output_hidden_states:\n",
      "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
      "\n",
      "        output = self.dropout(output_g if output_g is not None else output_h, training=training)\n",
      "\n",
      "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
      "        outputs = (tf.transpose(output, perm=(1, 0, 2)),)\n",
      "\n",
      "        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n",
      "            outputs = outputs + (new_mems,)\n",
      "\n",
      "        if self.output_hidden_states:\n",
      "            if output_g is not None:\n",
      "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
      "            else:\n",
      "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
      "            outputs = outputs + (hidden_states,)\n",
      "        if self.output_attentions:\n",
      "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
      "            outputs = outputs + (attentions,)\n",
      "\n",
      "        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4863 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 206 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 206 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "4864/4863 [==============================] - 740s 152ms/sample - loss: 0.4057\n",
      "Epoch 2/3\n",
      "4864/4863 [==============================] - 537s 110ms/sample - loss: 0.3719\n",
      "Epoch 3/3\n",
      "4864/4863 [==============================] - 537s 110ms/sample - loss: 0.3594\n",
      "validation score =  0.40159847656291614\n"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n",
    "\n",
    "valid_preds = []\n",
    "test_preds = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "    # will actually only do 2 folds (out of 5) to manage < 2h\n",
    "    if fold in [0, 2]:\n",
    "        print('''\n",
    "        FOLD {}\n",
    "        '''.format(fold+1))\n",
    "\n",
    "        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "        train_outputs = outputs[train_idx]\n",
    "\n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "        \n",
    "        K.clear_session()\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        with strategy.scope():\n",
    "            model = create_model()\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "            custom = CustomCallback(fold=fold)\n",
    "            model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        model.fit(train_inputs, train_outputs, epochs=3, batch_size=4,callbacks=[custom])\n",
    "        valid_preds.append(model.predict(valid_inputs))\n",
    "        test_preds.append(model.predict(test_inputs))\n",
    "        \n",
    "        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
    "        print('validation score = ', rho_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
