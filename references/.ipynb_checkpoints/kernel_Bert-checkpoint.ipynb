{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:51.780434Z",
     "start_time": "2020-01-18T12:27:50.296518Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# from bert.tokenization import bert_tokenization as tokenization\n",
    "import sys\n",
    "sys.path.append('../modules/')\n",
    "import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "import gc\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from tensorflow.keras.models import load_model\n",
    "import gc\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:51.784206Z",
     "start_time": "2020-01-18T12:27:51.781879Z"
    }
   },
   "outputs": [],
   "source": [
    "BERT_PATH = '../data/bert_en_uncased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:51.857794Z",
     "start_time": "2020-01-18T12:27:51.785780Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(BERT_PATH+'assets/vocab.txt', True)\n",
    "MAX_SEQUENCE_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:51.981041Z",
     "start_time": "2020-01-18T12:27:51.859355Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df_sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:51.986579Z",
     "start_time": "2020-01-18T12:27:51.982920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (6079, 41)\n",
      "test shape = (476, 11)\n"
     ]
    }
   ],
   "source": [
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.009447Z",
     "start_time": "2020-01-18T12:27:51.987855Z"
    }
   },
   "outputs": [],
   "source": [
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.033279Z",
     "start_time": "2020-01-18T12:27:52.011548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output categories:\n",
      "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "input categories:\n",
      "\t ['question_title', 'question_body', 'answer']\n"
     ]
    }
   ],
   "source": [
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask attention & Padding Tokens (input_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.096786Z",
     "start_time": "2020-01-18T12:27:52.037688Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Tokens by [SEP] (input_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.164269Z",
     "start_time": "2020-01-18T12:27:52.099207Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Tokens to IDs by Tokenizer (input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.231218Z",
     "start_time": "2020-01-18T12:27:52.166572Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.306253Z",
     "start_time": "2020-01-18T12:27:52.233850Z"
    }
   },
   "outputs": [],
   "source": [
    "def _trim_input(title, question, answer, max_sequence_length, \n",
    "                t_max_len=30, q_max_len=239, a_max_len=239):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "        \n",
    "        t = t[:t_new_len]\n",
    "        q = q[:q_new_len]\n",
    "        a = a[:a_new_len]\n",
    "    \n",
    "    return t, q, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conver to Bert inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.375259Z",
     "start_time": "2020-01-18T12:27:52.307965Z"
    }
   },
   "outputs": [],
   "source": [
    "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    \n",
    "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "    input_masks = _get_masks(stoken, max_sequence_length)\n",
    "    input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.407379Z",
     "start_time": "2020-01-18T12:27:52.377575Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        t, q, a = _trim_input(t, q, a, max_sequence_length)\n",
    "\n",
    "        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "        \n",
    "    return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.490432Z",
     "start_time": "2020-01-18T12:27:52.409975Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Spearman's rank correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.549125Z",
     "start_time": "2020-01-18T12:27:52.493106Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_spearmanr(trues, preds):\n",
    "    rhos = []\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        rhos.append(\n",
    "            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
    "    return np.mean(rhos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.615394Z",
     "start_time": "2020-01-18T12:27:52.551924Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n",
    "\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "        self.test_inputs = test_data\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "        self.test_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "        \n",
    "        rho_val = compute_spearmanr(\n",
    "            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n",
    "        \n",
    "        print(\"\\nvalidation rho: %.4f\" % rho_val)\n",
    "        \n",
    "#         self.model.save_weights(f'bert-base.h5py')\n",
    "        \n",
    "        if self.fold is not None:\n",
    "#             self.model.save_weights(f'../saved_models/bert-base-{fold}-{epoch}.h5py')\n",
    "            tf.saved_model.save(self.model, f'../saved_models/hist_bert_ep_fcv/hist_{fold+1}_bert_ep{epoch+1}_fcv')\n",
    "        \n",
    "        self.test_predictions.append(\n",
    "            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.684617Z",
     "start_time": "2020-01-18T12:27:52.617420Z"
    }
   },
   "outputs": [],
   "source": [
    "def bert_model():\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n",
    "    input_masks = tf.keras.layers.Input(\n",
    "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n",
    "    input_segments = tf.keras.layers.Input(\n",
    "        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n",
    "    \n",
    "    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n",
    "    \n",
    "    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.707887Z",
     "start_time": "2020-01-18T12:27:52.687096Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_predict(model, train_data, valid_data, test_data, \n",
    "                      learning_rate, epochs, batch_size, loss_function, fold, times):\n",
    "        \n",
    "    custom_callback = CustomCallback(\n",
    "        valid_data=(valid_data[0], valid_data[1]), \n",
    "        test_data=test_data,\n",
    "        batch_size=batch_size,\n",
    "        fold=fold)\n",
    "#     save_callback = keras.callbacks.ModelCheckpoint(\n",
    "#         filepath='../saved_models/hist_'+str(times)+'_bert', monitor='loss',\n",
    "#         verbose=0, save_best_only=True,\n",
    "#         save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    model.fit(train_data[0], train_data[1], epochs=epochs, \n",
    "              batch_size=batch_size, callbacks=[\n",
    "                  custom_callback,\n",
    "#                   save_callback\n",
    "              ])\n",
    "    \n",
    "    return custom_callback, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.757055Z",
     "start_time": "2020-01-18T12:27:52.710204Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:27:52.827970Z",
     "start_time": "2020-01-18T12:27:52.758600Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input & Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:28:20.840632Z",
     "start_time": "2020-01-18T12:27:52.830598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:25, 240.80it/s]\n",
      "476it [00:02, 231.15it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:28:20.884432Z",
     "start_time": "2020-01-18T12:28:20.842222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6079, 512)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(inputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:28:20.888609Z",
     "start_time": "2020-01-18T12:28:20.886547Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T12:28:20.917599Z",
     "start_time": "2020-01-18T12:28:20.889772Z"
    }
   },
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:12:38.433113Z",
     "start_time": "2020-01-18T12:28:20.919349Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        FOLD 1\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.4060"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation rho: nan\n",
      "WARNING:tensorflow:From /home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_0_bert_ep0/assets\n",
      "5471/5471 [==============================] - 426s 78ms/sample - loss: 0.4059\n",
      "Epoch 2/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3743\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_0_bert_ep1/assets\n",
      "5471/5471 [==============================] - 411s 75ms/sample - loss: 0.3743\n",
      "Epoch 3/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3587\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_0_bert_ep2/assets\n",
      "5471/5471 [==============================] - 412s 75ms/sample - loss: 0.3587\n",
      "Epoch 4/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3448\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_0_bert_ep3/assets\n",
      "5471/5471 [==============================] - 410s 75ms/sample - loss: 0.3448\n",
      "Epoch 5/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3299\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_0_bert_ep4/assets\n",
      "5471/5471 [==============================] - 409s 75ms/sample - loss: 0.3299\n",
      "\n",
      "        FOLD 2\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.4099\n",
      "validation rho: 0.3306\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_1_bert_ep0/assets\n",
      "5471/5471 [==============================] - 433s 79ms/sample - loss: 0.4099\n",
      "Epoch 2/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3775\n",
      "validation rho: 0.3584\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_1_bert_ep1/assets\n",
      "5471/5471 [==============================] - 410s 75ms/sample - loss: 0.3774\n",
      "Epoch 3/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3621\n",
      "validation rho: 0.3733\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_1_bert_ep2/assets\n",
      "5471/5471 [==============================] - 413s 75ms/sample - loss: 0.3621\n",
      "Epoch 4/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3492\n",
      "validation rho: 0.3783\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_1_bert_ep3/assets\n",
      "5471/5471 [==============================] - 411s 75ms/sample - loss: 0.3492\n",
      "Epoch 5/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3355\n",
      "validation rho: 0.3825\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_1_bert_ep4/assets\n",
      "5471/5471 [==============================] - 414s 76ms/sample - loss: 0.3356\n",
      "\n",
      "        FOLD 3\n",
      "        \n",
      "Train on 5471 samples\n",
      "Epoch 1/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.4086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/yashima/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_2_bert_ep0/assets\n",
      "5471/5471 [==============================] - 431s 79ms/sample - loss: 0.4087\n",
      "Epoch 2/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3748\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_2_bert_ep1/assets\n",
      "5471/5471 [==============================] - 413s 76ms/sample - loss: 0.3748\n",
      "Epoch 3/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3589\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_2_bert_ep2/assets\n",
      "5471/5471 [==============================] - 414s 76ms/sample - loss: 0.3589\n",
      "Epoch 4/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3459\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_2_bert_ep3/assets\n",
      "5471/5471 [==============================] - 411s 75ms/sample - loss: 0.3459\n",
      "Epoch 5/5\n",
      "5464/5471 [============================>.] - ETA: 0s - loss: 0.3308\n",
      "validation rho: nan\n",
      "INFO:tensorflow:Assets written to: ../saved_models/hist_2_bert_ep4/assets\n",
      "5471/5471 [==============================] - 412s 75ms/sample - loss: 0.3308\n"
     ]
    }
   ],
   "source": [
    "# histories = []\n",
    "models = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "    # will actually only do 3 folds (out of 5) to manage < 2h\n",
    "    if fold < 10:\n",
    "        K.clear_session()\n",
    "        \n",
    "#         strategy = tf.distribute.MirroredStrategy()\n",
    "#         with strategy.scope():\n",
    "        print('''\n",
    "        FOLD {}\n",
    "        '''.format(fold+1))\n",
    "        \n",
    "        model = bert_model()\n",
    "\n",
    "        train_inputs = [inputs[i][train_idx] for i in range(3)]\n",
    "        train_outputs = outputs[train_idx]\n",
    "\n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "\n",
    "            # history contains two lists of valid and test preds respectively:\n",
    "            #  [valid_predictions_{fold}, test_predictions_{fold}]\n",
    "\n",
    "\n",
    "        history, model = train_and_predict(model, \n",
    "                                train_data=(train_inputs, train_outputs), \n",
    "                                valid_data=(valid_inputs, valid_outputs),\n",
    "                                test_data=test_inputs, \n",
    "                                learning_rate=1e-5, epochs=5, batch_size=4,\n",
    "                                loss_function='binary_crossentropy', fold=fold, times=fold+1)\n",
    "\n",
    "#         histories.append(history)\n",
    "        models.append(model)\n",
    "        del history, model, train_inputs, train_outputs\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:12:38.438236Z",
     "start_time": "2020-01-18T14:12:38.435472Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = load_model('../saved_models/hist_1/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
